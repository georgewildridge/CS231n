<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>CS231n by georgewildridge</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">CS231n</h1>
      <h2 class="project-tagline">Following Stanford&#39;s course &quot;Convolutional Neural Networks for Visual Recognition&quot;</h2>
      <a href="https://github.com/georgewildridge/CS231n" class="btn">View on GitHub</a>
      <a href="https://github.com/georgewildridge/CS231n/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/georgewildridge/CS231n/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <p>This is currently being revised and edited:</p>

<p>This will be a comprehensive review of everything I have learned following <a href="http://cs231n.stanford.edu/">CS231n</a>and completing assignment 1.  If you have questions or spot a mistake feel free to tweet me at <a href="https://twitter.com/Gwildridge">@Gwildridge</a>. For resources I used the <a href="http://cs231n.stanford.edu/syllabus.html">CS231n lectures and notes</a> (until the lectures were taken down) and I occasionally used <a href="http://networkflow.net/forum/19-stanford-cs231n-convolutional-neural-networks-for-visual-recognition/">this forum</a>. I was also assisted by a professor from my school who helped walk me through some of the more complex topics.
My goal in completing this assignment was to understand the concepts behind neural networks. I was able to implement a lot of the code myself with the aid of the course notes but I did have to resort to the forum in the beginning when it came to code vectorization as I just did not understand what was being asked… this is most likely because I have neither taken linear algebra or multi-variable calculus. Even still this was only after four or five hours of work. Nevertheless, as I came into contact with more and more vectorized code and began internalizing what exactly was going on I was able to become more independent as time progressed. That said I still desire to understand the code I received help with and that is in part the motivation behind this article… to prove to myself I truly do understand it.</p>

<h1>
<a id="introduction" class="anchor" href="#introduction" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h1>

<h2>
<a id="computer-vision" class="anchor" href="#computer-vision" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Computer Vision</h2>

<p>One particularly relevant application of deep learning and neural networks to this course is computer vision. Understanding how to give computers the benefit of site has been a key goal in the computer science community that came about in the very early days of vision. As researchers found, understanding computer vision is as much about understanding computers as it is about understanding human vision as that is inevitably what is trying to be replicated. Over the past fifty years, our understanding of human vision has shifted from a holistic approach to an hierarchical approach. Previously it was thought that the brain would store whole objects in a very specific part of the brain, now, with the hierarchical approach, it is believed that vision can be broken down into much smaller components then objects. Edges, for instance, are believed to define shapes so instead of storing an object somewhere in our brains we store a set of edges and lines that make up that object and only upon the triggering of those very specific edges and lines do we understand what an object is. With these specific edges and lines, humans are able to build complex 3d representations of objects within our head, which allows us to understand what objects are no matter their orientation, color, etc.  The overarching goal is to build this 3d model of objects within the computer. 
The first approach to solving vision was edge detection and then moved into segmentation for feature detection. An example of this would be detecting for specific shapes in an image, say the straight legs of a chair a certain distance apart. For almost sixty years, people attempted unsuccessfully to solve vision using this method. Now focus has shifted to deep learning as it has achieved promising results over the past couple years in the field of machine learning. This isn't to say that Deep Learning hadn't existed until recently, as the first neural networks were built in the 60's, but it is to say that very few focused on it as a solution because it was just too slow and computationally costly.</p>

<h2>
<a id="image-classification" class="anchor" href="#image-classification" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Image Classification</h2>

<p>Image classification means taking an image and transforming it into one of a fixed number of categories. This is easier said than done because of a number of environment variables like rotation, point of view, illumination, deformation, occlusion, background clutter and interclass variation. Over the past sixty years two approaches have been taken: the explicit approach and the data driven approach. An example of an explicit approach is feature detection (the search for two straight lines a certain distance from each other to classify a chair). The issue with this approach is that every time you want to classify a new object a new algorithm must be hard coded to look for certain characteristics, making this method incapable of major growth. Deep learning follows the latter approach, which involves collecting a very large dataset of images and labels and then training algorithms using machine learning. </p>

<h1>
<a id="assignment-1" class="anchor" href="#assignment-1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Assignment 1</h1>

<p>In this assignment I will implement the k-nearest neighbor classifier, a support vector machine, the softmax classifier, backpropagation (stochastic gradient descent) and a neural net. </p>

<h2>
<a id="k-nearest-neighbor-classifier" class="anchor" href="#k-nearest-neighbor-classifier" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>K-Nearest Neighbor Classifier</h2>

<p>This classifier literally compares an image for similarities to a dataset. After finding the most similar image/images in the dataset, it classifies the original image with the class of the similar image/images. By similar I am referring to the distances between the two images. Two different methods can be used to find the distance, the L1/Manhattan distance:</p>

<p><img src="https://raw.githubusercontent.com/georgewildridge/CS231n/master/Website%20Pictures/manhattan.png" alt="Manhattan distance"></p>

<p>And the L2/Euclidean distance:</p>

<p><img src="https://raw.githubusercontent.com/georgewildridge/CS231n/master/Website%20Pictures/euclidean.png" alt="Euclidean Distance"></p>

<p>The image with the smallest distance between the two is considered to be the most similar image. This is called the nearest neighbor classifier. The K-nearest neighbor classifier is very similar as instead of selecting just one most similar images, it selects K most similar images and then classifies the image based off the class that has the majority in the K images. </p>

<p>In this particular assignment we were asked to implement this classifier using the Euclidean/L2 distance in three different ways: with two for loops, with one for loop and with no for loops. This is designed to introduce us to vectorizing code. </p>

<h3>
<a id="two-loops" class="anchor" href="#two-loops" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Two Loops</h3>

<p>First I just convert the equation for Euclidean distance into numpy operation:</p>

<pre><code>num_test = X.shape[0]
num_train = self.X_train.shape[0]
dists = np.zeros((num_test, num_train))
for i in xrange(num_test):
    for j in xrange(num_train):
        dists[i,j] = np.sqrt(np.sum(np.square(X[i]-self.X_train[j])))
return dists
</code></pre>

<p>Where X[i] is the image we are attempting to find the class for and X_train[j] is the remainder of the images in the training set.</p>

<h3>
<a id="one-loop" class="anchor" href="#one-loop" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>One Loop</h3>

<p>Reducing the same operation to one loop:</p>

<pre><code>for i in xrange(num_test):
    dists[i,:] = np.sqrt(np.sum(np.square(X[i,:]-self.X_train),axis =1))
return dists
</code></pre>

<p>The important thing to understand in this case is what exactly something like dists[i,:] is doing. From playing around with it and from the <a href="http://cs231n.github.io/python-numpy-tutorial/">python tutorial</a>, I figured out that his would represent every column in row i. It is just a more efficient way to compare every picture to the dataset. </p>

<h3>
<a id="no-loops" class="anchor" href="#no-loops" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>No Loops</h3>

<p>Now we take it a step further, computing the Euclidean distance without loops:</p>

<pre><code>dists = np.sqrt((X**2).sum(axis=1)[:, np.newaxis] + (self.X_train**2).sum(axis=1) - 2 * X.dot(self.X_train.T))
return dists
</code></pre>

<p>To put this into perspective I will do it on a much simpler scale. Instead of dealing with these huge matrices, pretend we are just working with two vectors, q and p. To find the <a href="https://en.wikipedia.org/wiki/Euclidean_distance">Euclidean distance</a> between these two vectors we find the length of its distance vector:</p>

<p><img src="https://upload.wikimedia.org/math/c/5/4/c54c79bb419dec31d93260cb9207a1d5.png" alt="Euclidean length of distance vector"></p>

<p>Otherwise written as:</p>

<p><img src="https://upload.wikimedia.org/math/a/8/3/a8394d4ad1d858186f89be9c590a3ac8.png" alt="rewritten Euclidean length of distance vector"></p>

<p>Alright, now that we understand how to find the length of the two vector's distance vector, the same process is applied to the bigger matrices.</p>

<h2>
<a id="linear-classifier" class="anchor" href="#linear-classifier" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Linear Classifier</h2>

<p>Although this is not a direct part of the assignment, it is necessary to understand what exactly the Support Vector Machine and the Softmax classifier are doing. A linear classifier involves an input(an image in this case) and a set of initially arbitrary parameters known as weights. Here it is in its most basic form:</p>

<p><img src="https://raw.githubusercontent.com/georgewildridge/CS231n/master/Website%20Pictures/linear.png" alt="linear classifier"></p>

<p>Where x is the image's pixels (stretched into one row) and W is a one column matrix with as many rows as there are classes. When the two are multiplied together we get a score for each class. Basically we are computing a weighted sum of all the pixel values for each score.  Ideally when we pass an image through, the correct classifier will have the highest score; however, this is often not the case which is where the SVM and the Softmax classifier come in. These classifiers are used to interpret the scores and quantify how poorly our linear classifier is working.  can be used It is important to note that the scores will be score free, and when first training a linear classifier, selected at random. </p>

<h2>
<a id="support-vector-machine" class="anchor" href="#support-vector-machine" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Support Vector Machine</h2>

<p>Using the output of a linear classifier given random weights, the intent of the SVM is to gauge the degree of how wrong the classification is. It does this by repeatedly finding the difference between the lowest score and the correct score for each image.  With the output we can identify how to change the linear classifiers weights in order to improve classification on its next pass. That was a very high level overview, now to look at exactly what is happening:</p>

<p><img src="https://raw.githubusercontent.com/georgewildridge/CS231n/master/Website%20Pictures/SVM.png" alt="SVM"></p>

<p>Where Li is the loss for a single training example, x is a vector of images, w is a vector of weights, y is a vector of labels and delta represents the safety margin. 
The max is taken in order to clamp possible loss at zero so negative numbers will not have a bearing on the overall loss of the function. If negative loss was allowed to pass through, it would decrease the overall loss which can distort the overall loss and make it seem like we are closer to identifying the correct scores even if we aren't. 
For the overall loss we must average the loss over all the images:</p>

<p><img src="https://raw.githubusercontent.com/georgewildridge/CS231n/master/Website%20Pictures/overallloss.png" alt="Overall Loss"></p>

<p>Where N is the number of training examples. </p>

<p>In this part of the assignment we first compute the loss and the gradient with loops before implementing a vectorized version.</p>

<h3>
<a id="with-loops" class="anchor" href="#with-loops" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>With Loops:</h3>

<p>First off, I need to refer you to the <a href="http://cs231n.github.io/optimization-1/#analytic">course notes</a> as it goes through many aspects of this part of the assignment. Naturally it is a good idea to start by implementing the SVM loss function. After I implemented the loss function I also implemented something called  the regularization loss:
<img src="https://raw.githubusercontent.com/georgewildridge/CS231n/master/Website%20Pictures/regularizationloss.png" alt="Regularization Loss"></p>

<p>Regularization of the Weights(W).</p>

<p>As the notes explain, regularization is important because often times there will be multiple sets of weights that will correctly classify the images. However, it is important to not be optimizing for all of them and instead be optimizing for only one set of weights. This is where regularization comes in. All regularization does is discourage large weights by using an "elementwise quadratic penalty". We then incorporate this into the loss function along with some hyper parameter lamda which controls how much effect the regularization loss has:
<img src="https://raw.githubusercontent.com/georgewildridge/CS231n/master/Website%20Pictures/fullloss.png" alt="full loss"></p>

<p>Or fully expanded:
<img src="https://raw.githubusercontent.com/georgewildridge/CS231n/master/Website%20Pictures/svmexp.png" alt="expanded svm"></p>

<p>After trying unsuccessfully to get the right gradient, I checked the forum and it actually turns out that the equation they use for regularization loss is:
<img src="https://raw.githubusercontent.com/georgewildridge/CS231n/master/Website%20Pictures/regloss.png" alt="regularization loss"></p>

<p>Now to implement the code:</p>

<pre><code># compute the loss
num_classes = W.shape[1]  # how many classes
num_train = X.shape[0] # how many images are in the training dataset
loss = 0.0
for i in xrange(num_train):
    scores = X[i].dot(W) #linear classifier
    dT = np.zeros(W.shape) # create an array the same size as the weight matrix made entirely out of zeros
    correct_class_score = scores[y[i]]
    for j in xrange(num_classes):
        if j == y[i]: #importantly the loss is not calculated when you are comparing an image to itself as it would give a value of (delta) which would have a non universal affect on the overall loss. 
            continue
        margin = scores[j] - correct_class_score + 1 # note delta = 1
        if margin &gt; 0:
            loss += margin

# finding the overall loss
loss /= num_train


# Add regularization to the loss.
loss += 0.5 * reg * np.sum(W * W)

return loss
</code></pre>

<p>Now to include the gradient. As the course notes tell us the equation for the gradient of an SVM can be found by taking the gradient with respect to (wyi). The 1 in this equation is supposed to be an indicator function:</p>

<p><img src="https://raw.githubusercontent.com/georgewildridge/CS231n/master/Website%20Pictures/SVMgradient.png" alt="gradient"></p>

<p>It then goes on to tell us, and I quote, "when you’re implementing this in code you’d simply count the number of classes that didn’t meet the desired margin (and hence contributed to the loss function) and then the data vector (Xi) scaled by this number is the gradient".  The equation above is only the gradient with respect to the row of W that "corresponds with the correct class." Where J does not equal yi the gradient is:</p>

<p><img src="https://raw.githubusercontent.com/georgewildridge/CS231n/master/Website%20Pictures/svmgradient1.png" alt="svm grad"></p>

<p>Finally, we also must take the gradient of the regularization loss which comes out to:</p>

<p><img src="https://raw.githubusercontent.com/georgewildridge/CS231n/master/Website%20Pictures/gradreg.png" alt="gradient regularization"></p>

<p>So we will include the gradient:</p>

<pre><code># compute the loss and the gradient
num_classes = W.shape[1] 
num_train = X.shape[0]
loss = 0.0 
for i in xrange(num_train):
    scores = X[i].dot(W)
    dW = np.zeros(W.shape) #initializing the gradient
    correct_class_score = scores[y[i]]
    gradCounter = 0 #keeping track of the number of positive scores
    for j in xrange(num_classes):
        if j == y[i]:
            continue
        margin = scores[j] - correct_class_score + 1 
        if margin &gt; 0:
            loss += margin
            dW[:,j] = X[i] #updating the gradient for incorrect rows with the image
            gradCounter += 1
    #adding the gradient to the column of the correct class
    dW[:,y[i]] = -(gradCounter  * X[i])

#regularize the gradient
dW = reg*W 
Full gradient:
dW += dW/num_train 

# We want the loss to be an average of all the training examples so we divide by num_train
loss = loss/num_train

# Add regularization to the loss.
loss += 0.5 * reg * np.sum(W * W)  

return loss, dW
</code></pre>

<h3>
<a id="vectorized" class="anchor" href="#vectorized" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Vectorized</h3>

<p>Now for the hard part: vectorizing the equations we just used above. This is one of those cases where I resorted to <a href="http://networkflow.net/forum/19-stanford-cs231n-convolutional-neural-networks-for-visual-recognition/">the forum</a> for understanding after four or five hours of unsuccessful attempts. To be as clear as possible, this is not my code.  Although I have partial explanations in the comments, when I have time I plan on putting matrices next to this code for each step to show exactly what is happening. </p>

<pre><code>loss = 0.0
num_train = X.shape[0]
num_classes = W.shape[1]
dW = np.zeros(W.shape) # initialize the gradient as zero


#Vectorized version of the SVM loss
score = X.dot(W) #linear classifier

y_pred = score[range(score.shape[0]),y]  

#SVM loss
margins = score - y_pred[:,None] + 1 #
margins[range(score.shape[0]),y] = 0
margins = np.maximum(np.zeros(margins.shape),margins)
loss = np.sum(margins) 
loss /= num_train #average loss
loss += 0.5 * reg * np.sum(W*W) #regularization


#Vectorized version of the gradient
Mc = (margins&gt;0).astype(float) 
Mc[np.arange(num_train), y] = -1
dW = np.dot(X.T, Mc)/ num_train + reg * W

return loss, dW
</code></pre>

<h2>
<a id="optimization" class="anchor" href="#optimization" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Optimization</h2>

<p>Through using either the SVM or the Softmax classifier (which I will address later) we can obtain the loss; however, we lack a way of improving the classifier. One method is to randomly increase and decrease the weights and use the weights that yield the smallest loss. Or, as this is  terrible method, we could follow the slope. </p>

<h3>
<a id="numerical-gradient" class="anchor" href="#numerical-gradient" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Numerical Gradient</h3>

<p>The numerical gradient is found by taking the slope in a bunch of different directions. Through this we are finding the gradient, or what direction is uphill, however we are trying to decrease our values so we will take the negative of the gradient and take a small step down hill.  This method is effective but it is incredibly slow and computationally expensive… it is the reason people focused on feature detection for 50 years. Instead we will predominately use a technique called backpropagation which is much less computationally expensive. However, the numerical gradient is still utilized to perform gradient checks on backpropagation as it is much more reliable.  </p>

<h3>
<a id="analytic-gradient-backpropagation" class="anchor" href="#analytic-gradient-backpropagation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Analytic Gradient: Backpropagation</h3>

<p>To understand backpropagation you have to understand the chain rule with partial derivatives. After that it is rather straightforward as backpropagation is the recursive application of chain rule through a computational graph to find the influence of every intermediate value in the graph on the final loss function. This enables you to find the effects that the inputs have on the output of the loss function. There is an entire section of the <a href="http://cs231n.github.io/optimization-1/">course notes</a> which explains this.</p>

<h3>
<a id="stochastic-gradient-descent" class="anchor" href="#stochastic-gradient-descent" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Stochastic Gradient Descent</h3>

<p>In the SVM part of the assignment, they also ask us to write a few lines of code in the linear classifier file to implement stochastic gradient descent. The course notes inform us that stochastic gradient descent explicitly refers to doing gradient descent (changing the values of the weights in an effort to improve classification accuracy) with only one example. However, stochastic gradient descent is often used to refer to mini-batch gradient descent as well, which trains on multiple examples. Technically we are implementing mini-batch gradient descent here. 
First they ask us to sample some batch size elements from the training data and the corresponding labels. Further they give us a hint to use the np.random.choice function to generate indices. With all this in mind, it becomes pretty he to do this part.</p>

<pre><code> z = np.random.choice(num_train, batch_size,replace=True) # take a random batch, size batch_size, from the array num_train
 X_batch = X[z] # retrieve the images and their values
 y_batch = y[z]
</code></pre>

<p>Next they ask us to update the weights using the gradient and the learning rate. The learning rate is just another word for step size, this variable adjusts how quickly we are attempting to optimize. We have already done something very similar to this in the SVM implementation:</p>

<pre><code>self.W += -learning_rate * grad
</code></pre>

<p>Finally they ask us to implement the predict method to predict the labels for the data points:</p>

<pre><code>y_pred = np.argmax(X.dot(self.W), axis=1) #returns the indices of the maximum value of the interior function, which is just the linear classifier. 
</code></pre>

<h2>
<a id="softmax-classifier" class="anchor" href="#softmax-classifier" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Softmax Classifier</h2>

<p>Again using the scores outputted from the linear classifier given random weights, the softmax classifier interprets the scores as the unnormalized log probabilities of the classes (Andrej mentioned the reason behind this interpretation is very complex). So we first exponentiate the scores, normalize them and then take the -log to retrieve the probabilities of the classes. We then attempt to maximize the log likelihood and minimize the -log likelihood of the true class. Two sets of course notes that were really helpful to me when I implemented this can be found <a href="http://cs231n.github.io/neural-networks-case-study/">here</a> and <a href="http://cs231n.github.io/linear-classify/">here</a>. Here is an equation for what I just described.</p>

<p><img src="https://raw.githubusercontent.com/georgewildridge/CS231n/master/Website%20Pictures/softmax.png" alt="softmax"></p>

<p>Where Li is the loss for a single example, f is the array of class scores for a single example. </p>

<p>The gradient of which would be: </p>

<p><img src="https://raw.githubusercontent.com/georgewildridge/CS231n/master/Website%20Pictures/softmaxgradient.png" alt="gradient"></p>

<p>Where P is a vector of the normalized probabilities. The notes also emphasize a point on ensuring numerical stability as we could potentially be dealing with large numbers. They suggest we implement an equation like this:</p>

<p><img src="https://raw.githubusercontent.com/georgewildridge/CS231n/master/Website%20Pictures/numericalStability.png" alt="Numerical Stability"></p>

<p>Further, the notes go on to tell us that it is common choice for C is to set:</p>

<p><img src="https://raw.githubusercontent.com/georgewildridge/CS231n/master/Website%20Pictures/numstabmax.png" alt="Numerical Stability"></p>

<p>Here is the implementation with loops:</p>

<h3>
<a id="with-loops-1" class="anchor" href="#with-loops-1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>With Loops</h3>

<pre><code>#Initialize the loss and gradient to zero.
loss = 0.0
dW = np.zeros(W.shape)

num_classes = W.shape[1]
num_train = X.shape[0]
for i in xrange(num_train):
    scores = X[i].dot(W) #linear classifier
    scores -= np.max(scores)  #numerical stability 
    prob = np.exp(scores)/np.sum(np.exp(scores), axis=1, keepdims = True) #exponentiation and normalization
    loss += -np.log(prob[y[i]])  #-log
    for j in xrange(num_classes):
        dW[:,j] += (prob[j]-(j == y[i])) * X[i] #updating the gradient
loss /= num_train #overall loss
loss += 0.5 * reg * np.sum(W * W) #regularization
dW = (1.0/num_train)*dW 
dW += reg*W #weight regularization

return loss, dW
</code></pre>

<p>###Vectorized
Again I struggled for hours trying to figure out how to do this myself before resorting to the <a href="http://networkflow.net/forum/19-stanford-cs231n-convolutional-neural-networks-for-visual-recognition/">forum</a>. If it is at all unclear, this is not my code; however, I did spend a great deal of time endeavoring to understand the motivation behind every line. Although I have partial explanations in the comments, when I have time I plan on putting matrices next to this code for each step to show exactly what is happening. 
    scores = X.dot(W) # linear classifer
    scores -= np.max(scores,axis=1).reshape(num_train,1)  </p>

<pre><code>#softmax with numerical stability
prob = np.exp(scores)/np.reshape(np.sum(np.exp(scores),axis=1),(num_train,1)
loss = -np.sum(np.log(prob[(range(num_train),y)])) 
loss /= num_train
#final loss with weight regularization
loss += 0.5 * reg * np.sum(W * W)

#calculating the gradient w/ weight regularization
prob[(range(num_train),y)] = prob[(range(num_train),y)] - 1
dW = (1.0/num_train) * np.dot(X.T,prob) + reg * W

return loss, dW
</code></pre>

<h2>
<a id="two-layer-neural-network" class="anchor" href="#two-layer-neural-network" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Two-Layer Neural Network</h2>

<p>Looking at the equation of a neural network is probobly the best way to understand what exactly it is. Here's an equation for a 3 layer Neural Network:</p>

<p><img src="https://raw.githubusercontent.com/georgewildridge/CS231n/master/Website%20Pictures/theelayer.png" alt="3 layer"></p>

<p>Now a two layer:</p>

<p><img src="https://raw.githubusercontent.com/georgewildridge/CS231n/master/Website%20Pictures/twolayer.png" alt="2 layer"></p>

<p>Finally a linear classifier:</p>

<p><img src="https://raw.githubusercontent.com/georgewildridge/CS231n/master/Website%20Pictures/linear.png" alt="Linear classifier"></p>

<p>It is just a matter of embedding linear classifiers within each other, allowing for multiple sets of weights which means more detail about the image. In this part of the assignment we are asked to implement a two layer neural network. </p>

<h3>
<a id="forward-pass-and-computing-the-loss" class="anchor" href="#forward-pass-and-computing-the-loss" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Forward Pass and Computing the Loss</h3>

<p>The first three lines of code are simply executing the equation for the two layer neural net described above. </p>

<pre><code>layer1 = np.dot(X,W1) + b1 
layer2 = np.maximum(0,layer1)
scores = np.dot(layer2,W2) + b2
</code></pre>

<p>This is just the soft max loss with some minor adjustments as we are dealing with two sets of weights. Notably the hidden layer uses a "ReLU" non-linearity. This is function just means to take the maximum of 0 and x.</p>

<pre><code>#softmax loss
scores -= np.max(scores,axis=1)[:,np.newaxis]
prob = np.exp(scores)/np.sum(np.exp(scores),axis=1)[:,np.newaxis]
loss = -np.sum(np.log(prob[(xrange(N),y)]))
loss /= N
loss += 0.5 * reg * np.sum(W1 * W1)
loss += 0.5 * reg * np.sum(W2 * W2)
</code></pre>

<h3>
<a id="backward-pass-computing-gradients" class="anchor" href="#backward-pass-computing-gradients" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Backward Pass (computing gradients)</h3>

<p>The backward pass was more challenging; however, I was able to figure It out using the course notes on <a href="http://cs231n.github.io/optimization-2/">optimization</a> and <a href="http://cs231n.github.io/neural-networks-case-study/">neural networks</a>. 
If the ReLU layer just equals:</p>

<p><img src="https://raw.githubusercontent.com/georgewildridge/CS231n/master/Website%20Pictures/relu.png" alt="relu"></p>

<p>Then its gradient is:</p>

<p><img src="https://raw.githubusercontent.com/georgewildridge/CS231n/master/Website%20Pictures/relugradient.png" alt="relugradient"></p>

<p>As the first layer is just the softmax function again its gradient is:</p>

<p><img src="https://raw.githubusercontent.com/georgewildridge/CS231n/master/Website%20Pictures/softmaxgradient.png" alt="softmaxgradient"></p>

<p>Where P is a vector of normalized probabilities. Finally this step also calls for backpropagation, which is just the application of the chain rule:</p>

<pre><code>dscores = prob #softmax
dscores[xrange(N),y] -= 1 #gradient of softmax
dscores /= N 

#backpropagating the gradient
grads['W2'] = np.dot(layer2.T, dscores) + reg * W2
grads['b2'] = np.sum(dscores, axis=0)

dscores1 = np.dot(dscores, W2.T) * (layer1&gt;0)  #gradient of ReLU function

#backpropagating the gradient 
grads['W1'] = np.dot(X.T, dscores1) + reg * W1
grads['b1'] = np.sum(dscores1, axis = 0)     
</code></pre>

<h1>
<a id="future-work" class="anchor" href="#future-work" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Future Work</h1>

<p>When I reached this part of the course the lecture videos were taken down, as it took me an incredible amount of time to do this assignment with the lecture videos, my teacher and I decided to focus on learning to work with deep learning libraries like torch as it will enable me to use deep learning for more practical applications. Here is a link to that repo and what exactly I finished my term with. However, I have not given up on this course, I still feel I have a tremendous amount to learn from it. But from my initial experience, I hope to revisit it sometime next year after I have taken linear algebra and multivariable calculus so I am more capable of working through it independently. </p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/georgewildridge/CS231n">CS231n</a> is maintained by <a href="https://github.com/georgewildridge">georgewildridge</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
