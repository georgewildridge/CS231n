<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>CS231n by georgewildridge</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">CS231n</h1>
      <h2 class="project-tagline">Following Stanford&#39;s course &quot;Convolutional Neural Networks for Visual Recognition&quot;</h2>
      <a href="https://github.com/georgewildridge/CS231n" class="btn">View on GitHub</a>
      <a href="https://github.com/georgewildridge/CS231n/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/georgewildridge/CS231n/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <p>This is currently being revised and edited:</p>

<p>This will be a comprehensive review of everything I have learned following <a href="http://cs231n.stanford.edu/">CS231n</a>along with a walk through of my code.  I will work from the big picture down to the nitty gritty of the code. If you have questions or spot a mistake feel free to tweet me at <a href="https://twitter.com/Gwildridge">@Gwildridge</a>. A brief disclaimer that I am a high school student and although the information on this page is correct to the best of my knowledge, I would not consider myself all-knowing on this material. For resources I used the <a href="http://cs231n.stanford.edu/syllabus.html">CS231n lectures and notes</a>. I was using the lectures until they were taken down. To get through parts of the assignment used a forum that was posted under one of the lectures; that I have been unable to find since the comments disappeared with the video. I am doing my best to locate the forum so I can properly cite it. I also got a lot of aid in my understanding from a professor from my school.  For the majority of my work, my goal was to implement the function with loops and prove to myself I understand it. Now I also spent a huge amount of time trying to vectorize the equations; however, this step became exponentially harder for me as I have not taken linear algebra or multivariable calculus. Especially in the earlier cases, when after four or five hours of work I still felt no closer to knowing exactly what I was trying to do I would refer to my mythical lost resource for understanding. That said I still desired to understand and that is part of my motivation for creating this  blog post as I would like to express what I have learned in my own words which should help me reinforce the idea that I really do understand what I have been looking at. </p>

<h1>
<a id="introduction" class="anchor" href="#introduction" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h1>

<h2>
<a id="computer-vision" class="anchor" href="#computer-vision" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Computer Vision</h2>

<p>Computer vision is the problem that Deep Learning is attempting to solve and I am attempting to learn about. The approach to understanding this has also been an approach to understanding human vision. Over the past fifty years, our understanding of human vision has been widdled down from a holistic approach where some part of our brain stores what a chair looks like to a hierarchical understanding. Hierarchical vision revolves around the understanding that our brain sees edges and that these edges define the shape. So instead of storing a chair somewhere in our brains we store a set of edges and lines that make up a chair and our brain knows which objects are which upon recognition. The overarching goal is to build a 3d model of objects within the computer. 
The first approach to solving vision was edge detection nad then moved into segmentation for feature detection. For almost sixty years, people attempted to solve vision using this method… until very recently when deep leaning began being used. Now this isn't to say that Deep Learning hadn't existed until recently, it is to say that very few focused on it as a solution because it was too computationally costly. As of 2010 deep learning was a gain popularized. </p>

<h2>
<a id="image-classification" class="anchor" href="#image-classification" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Image Classification</h2>

<p>Image classification is taking an image and transform it into one of a fixed number of categories. Vision is challenging because of a number of environment variables like rotation, point of view, illumination, deformation, occlusion, background clutter and finally interclass variation. There are two approaches, the explicit approach and the data driven approach. An example of the explicit approach is the feature detection, or the association of certain characteristics with certain objects. The issue with this approach is that every time you want to classify a new object a new algorithm must be hard coded to look for those characteristics, making this method incapable of major growth. Instead, a data driven has been taken with deep learning. This involves collecting a very large dataset of images and labels. Then machine learning is used to train and image classifier on the data prior to evaluating the classifier on the test images. It is this classifier I will address next. </p>

<h1>
<a id="assignment-1" class="anchor" href="#assignment-1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Assignment 1</h1>

<p>In this assignment I will implement the k-nearest neighbor classifier, a support vector machine, the softmax classifier, backpropagation, a neural net and generate some features. </p>

<h2>
<a id="k-nearest-neighbor-classifier" class="anchor" href="#k-nearest-neighbor-classifier" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>K-Nearest Neighbor Classifier</h2>

<p>This classifier works by comparing an image for similarities to ever other image in the dataset, and is classified based of the image(s)' class that it is most similar to. By similar I am refering to the the distance's between the two images. Two different methods can be used to find the distance, the L1/Manhattan distance:
<img src="link" alt="Manhattan distance">
And the L2/Euclidean distance:
<img src="link" alt="Euclidean Distance">
The image with the smallest distance between the two is considered to be the most similar image. However, what I just described was the nearest neighbor classifier. The K-nearest neighbor classifier is very similar. Instead of just stating the image with the smallest number is the most similar, K of the most similar images are retrieved before selecting the class that the majority of the K images have. </p>

<p>In this particular assignment we were asked to implement this classifier using the Euclidean/L2 distance in three different ways: with two for loops, with one for loop and with no for loops. This is designed to introduce us to running loops in C instead of python.</p>

<h3>
<a id="two-loops" class="anchor" href="#two-loops" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Two Loops</h3>

<p>Converting the equation into numpy operations:</p>

<pre><code>num_test = X.shape[0]
num_train = self.X_train.shape[0]
dists = np.zeros((num_test, num_train))
for i in xrange(num_test):
    for j in xrange(num_train):
        dists[i,j] = np.sqrt(np.sum(np.square(X[i]-self.X_train[j])))
return dists
</code></pre>

<p>Where X[i] is the image we are attempting to find the class for and X_train[j] is the remainder of the images in the training set.</p>

<h3>
<a id="one-loop" class="anchor" href="#one-loop" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>One Loop</h3>

<p>Reducing the same operation to one loop</p>

<pre><code>for i in xrange(num_test):
    dists[i,:] = np.sqrt(np.sum(np.square(X[i,:]-self.X_train),axis =1))
return dists
</code></pre>

<p>The important thing to understand in this case is what exactly something like dists[i,:] is doing. From playing around with it and from the <a href="http://cs231n.github.io/python-numpy-tutorial/">python tutorial</a>, I figured out that his would represent every column in row i. So basically I am saying that every column in row i of dists is equal to the Euclidean distance of every column of row i minus every image in the dataset. </p>

<h3>
<a id="no-loops" class="anchor" href="#no-loops" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>No Loops</h3>

<p>Now we take it a step further, computing the euclidean distance without loops:</p>

<pre><code>dists = np.sqrt((X**2).sum(axis=1)[:, np.newaxis] + (self.X_train**2).sum(axis=1) - 2 * X.dot(self.X_train.T))
return dists
</code></pre>

<p>To put this into perspective I will do it on a much simpler scale. Instead of dealing with these huge matricies, pretend we are just working with two vectors, q and p. To find the <a href="https://en.wikipedia.org/wiki/Euclidean_distance">Euclidean distance</a> between these two vectors we find the length of its distance vector:</p>

<p><img src="https://upload.wikimedia.org/math/c/5/4/c54c79bb419dec31d93260cb9207a1d5.png" alt="Euclidean length of distance vector"></p>

<p>Otherwise written as:</p>

<p><img src="https://upload.wikimedia.org/math/a/8/3/a8394d4ad1d858186f89be9c590a3ac8.png" alt="rewritten Euclidean length of distance vector"></p>

<p>Alright, now that we understand how to find the length of the two vector's distance vector, the same process is applied to the bigger matrices and that’s it. </p>

<h2>
<a id="linear-classifier" class="anchor" href="#linear-classifier" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Linear Classifier</h2>

<p>Although this is not a direct part of the assignment, it is necessary to understand what exactly the Support Vector Machine and the Softmax classifer are doing… so bear with me. A linear classifier involves an input (an image in this case) and a set of initially arbitrary parameters known as weights. Then we have some function. Here it is in its most basic form:
<img src="link" alt="linear classifier">
Where x is the image's pixels (stretched into one column) and W is a one column matrix with as many rows as classes. When the two are multiplied together we get a score for each class. Basically we are computing a weighted sum of all the pixel values for each score. Ideally when we pass an image through, the correct classifier will have the lowest score; however, this is often not the case which is where the SVM and the softmax classifer come in. </p>

<h2>
<a id="support-vector-machine" class="anchor" href="#support-vector-machine" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Support Vector Machine</h2>

<p>Using the output of a linear classifier given random weights, the intent of the SVM is to gauge  the degree of how wrong the classification is. It does this by repeatedly finding the difference between the lowest score and the correct score for each image.  With the output we can identify how to change the linear classifiers weights in order so the linear classifier will improve in its next pass. That was a very high level overview, now to look at exactly what is happening:
<img src="link" alt="SVM">
Where S is a vector of class scores and 1 represents the safety margin. The scores are scale free and selected randomly. The max is taken in order to clamp possible loss at zero so negative numbers will not have a bearing on the overall loss of the function. If negative loss was allowed to pass through, it would decrease the overall loss which can distort the overall loss and make it seem like we are closer to identifying the correct scores even if we aren't. </p>

<p>In this part of the assignment we first compute the loss and the gradient with loops before implementing a vectorized version.</p>

<h3>
<a id="with-loops" class="anchor" href="#with-loops" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>With Loops:</h3>

<p>First off, I need to refer you to the <a href="http://cs231n.github.io/optimization-1/#analytic">course notes</a> as it goes through many aspects of this part of the assignment. Naturally it is a good idea to first start by implementing the SVM loss function at a single point. The equation for this looks like this:</p>

<p>And the code would look like this:</p>

<pre><code># compute the loss and the gradient
num_classes = W.shape[1]  # how many classes
num_train = X.shape[0] # how many images are in the training dataset
loss = 0.0
for i in xrange(num_train):
    scores = X[i].dot(W) #linear classifier
    dT = np.zeros(W.shape) # create an array the same size as the weight matrix made entirely out of zeros
    correct_class_score = scores[y[i]]
    for j in xrange(num_classes):
        if j == y[i]: #importantly the loss is not calculated when you are comparing an image to itself as it would give a value of (delta) which would have a non universal affect on the overall loss. 
            continue
        margin = scores[j] - correct_class_score + 1 # note delta = 1
        if margin &gt; 0:
            loss += margin
# Right now the loss is a sum over all training examples, but we want it
# to be an average instead so we divide by num_train.
loss /= num_train

# Add regularization to the loss.
loss += 0.5 * reg * np.sum(W * W)

return loss
</code></pre>

<p>From here it is relatively simple to incorporate the gradient. As the course notes tell us the equation for the gradient of an SVM can be found by taking the gradient with respect to (wyi):</p>

<p>It then goes on to tell us, and I quote, "when you’re implementing this in code you’d simply count the number of classes that didn’t meet the desired margin (and hence contributed to the loss function) and then the data vector (Xi) scaled by this number is the gradient".  So that is what we will do in the code:</p>

<pre><code># compute the loss and the gradient
num_classes = W.shape[1] 
num_train = X.shape[0]
loss = 0.0 
for i in xrange(num_train):
    scores = X[i].dot(W)
    dW = np.zeros(W.shape) #initializing the gradient
    correct_class_score = scores[y[i]]
    gradCounter = 0
    for j in xrange(num_classes):
        if j == y[i]:
            continue
        margin = scores[j] - correct_class_score + 1 
        if margin &gt; 0:
            loss += margin
            dW[:,j] = X[i] #updating the gradient for incorrect rows
            gradCounter += 1


#adding the gradient to the column of the correct class
dW[:,y[i]] = -(gradCounter  * X[i])


dW = reg*W + (dW/num_train) #regularize the weights

# We want the loss to be an average of all the training examples so we divide by num_train
loss = loss/num_train

# Add regularization to the loss.
loss += 0.5 * reg * np.sum(W * W)  

return loss, dW
</code></pre>

<h3>
<a id="vectorized" class="anchor" href="#vectorized" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Vectorized</h3>

<p>Now for the hard part. Vectorizing the equations we just used above. This is one of those cases where I resorted to the forums for understanding after four or five hours.  Although I lost the forum, I did find a great resource when I searched for it on <a href="https://bruceoutdoors.wordpress.com/2016/05/06/cs231n-assignment-1-tutorial-q2-training-a-support-vector-machine/">this blog</a>. On a side note, in taking this course I also learned a heck of a lot about python and numpy. </p>

<p>So I am going to start from the other way around this time, in the manner that I was forced to approach this problem:</p>

<pre><code>loss = 0.0
num_train = X.shape[0]
num_classes = W.shape[1]
dW = np.zeros(W.shape) # initialize the gradient as zero

score = X.dot(W) #linear classifier

y_pred = score[range(score.shape[0]),y]
print  score.shape[0]
margins = score - y_pred[:,None] + 1
#print margins
margins[range(score.shape[0]),y] = 0
# print margins[0]
margins = np.maximum(np.zeros(margins.shape),margins)

loss = np.sum(margins)
loss /= num_train
loss += 0.5 * reg * np.sum(W*W)

Mc = (margins&gt;0).astype(float) 
Mc[np.arange(num_train), y] = -1
dW = np.dot(X.T, Mc)/ num_train + reg * W

return loss, dW
</code></pre>

<h2>
<a id="optimization" class="anchor" href="#optimization" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Optimization</h2>

<p>Through using either the SVM or the Softmax classfier we can obtain the loss, the issue of incrementing or decrementing the weights must now be addressed. One method is to randomly increase and decrease the weights and use the weights that yield the smallest loss. Or, as this is  terrible method, we could follow the slope. </p>

<h3>
<a id="numerical-gradient" class="anchor" href="#numerical-gradient" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Numerical Gradient</h3>

<p>Basically we could compute what we will come to call an analytic gradient by taking the slope in a bunch of directions through taking a series of tiny steps. Through this we are finding the gradient, or what direction is uphill, however we are trying to decrease our values so we will take the - of the gradient and take a small step down hill.   This method is effective but it is incredibly slow and computationally expensive… it is the reason people focused on feature detection for 50 years. Instead a technique called backpropagation can be used to expedite the process and make it much less computationally expensive. </p>

<h3>
<a id="analytic-gradient-backpropagation" class="anchor" href="#analytic-gradient-backpropagation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Analytic Gradient: Backpropagation</h3>

<p>To understand backpropagation you have to understand the chain rule with partial derivatives. After that it is rather straightforward as backpropagation is the recursive application of chain rule through a computational graph to find the influence of every intermediate value in the graph on the final loss function. Finding the effects that the inputs have on the output of the loss function.</p>

<h3>
<a id="stochastic-gradient-descent" class="anchor" href="#stochastic-gradient-descent" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Stochastic Gradient Descent</h3>

<pre><code>def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=100, batch_size=200, verbose=False)
    num_train, dim = X.shape
    num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes
    if self.W is None:
        # lazily initialize Ws
        self.W = 0.001 * np.random.randn(dim, num_classes)
    # Run stochastic gradient descent to optimize W
    loss_history = []
    for it in xrange(num_iters):
         X_batch = None
         y_batch = None

        i = np.random.choice(num_train, batch_size,replace=True)
        X_batch = X[i]
        y_batch = y[i]

    # evaluate loss and gradient
    loss, grad = self.loss(X_batch, y_batch, reg)
    loss_history.append(loss)

    # perform parameter update
    self.W += -learning_rate * grad

    if verbose and it % 100 == 0:
        print 'iteration %d / %d: loss %f' % (it, num_iters, loss)

    return loss_history
</code></pre>

<h2>
<a id="softmax-classifier" class="anchor" href="#softmax-classifier" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Softmax Classifier</h2>

<p>Again using the scores outputted from the linear classifier given random weights, the softmax classifier interprets the scores as the unnormalized log probabilities of the classes (Andrej mentioned the reason behind this interpretation is very complex). So we first exponentiate the scores, normalize them and then take the -log to retrieve the probabilities of the classes. We then attempt to maximize the log likelihood and minimize the -log likelihood of the true class?</p>

<h3>
<a id="with-loops-1" class="anchor" href="#with-loops-1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>With Loops</h3>

<pre><code> Initialize the loss and gradient to zero.
loss = 0.0
dW = np.zeros_like(W)

num_classes = W.shape[1]
num_dim = W.shape[0]
num_train = X.shape[0]
prob = np.zeros((num_classes,1))
for i in xrange(num_train):
    scores = X[i].dot(W)
    scores -= np.max(scores) 
    prob = np.exp(scores)/np.sum(np.exp(scores))
    loss += -np.log(Pi[y[i]])
    prob[y[i]] -= 1 
    dW += np.dot(np.reshape(X[i],(num_dim,1)), np.reshape(prob,(1,num_classes)))

loss /= num_train
loss += 0.5 * reg * np.sum(W * W)
dW = (1.0/num_train)*dW + reg*W

return loss, dW
</code></pre>

<h3>
<a id="vectorized-1" class="anchor" href="#vectorized-1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Vectorized</h3>

<pre><code>scores = X.dot(W)
scores -= np.max(scores,axis=1).reshape(num_train,1)
prob = np.exp(scores)/np.reshape(np.sum(np.exp(scores),axis=1),(num_train,1))
loss = -np.sum(np.log(P[(range(num_train),y)]))
loss /= num_train
loss += 0.5 * reg * np.sum(W * W)

prob[(range(num_train),y)] = prob[(range(num_train),y)] - 1
dW = (1.0/num_train) * np.dot(X.T,P) + reg * W

return loss, dW
</code></pre>

<h2>
<a id="two-layer-neural-network" class="anchor" href="#two-layer-neural-network" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Two-Layer Neural Network</h2>

<h3>
<a id="forward-pass-and-computing-the-loss" class="anchor" href="#forward-pass-and-computing-the-loss" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Forward Pass and Computing the Loss</h3>

<pre><code>l1 = X.dot(W1) + b1.T
l2 = relu(l1).dot(W2) + b2.T
scores = l2
</code></pre>

<h3>
<a id="backward-pass-computing-gradients" class="anchor" href="#backward-pass-computing-gradients" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Backward Pass (computing gradients)</h3>

<h2>
<a id="image-features" class="anchor" href="#image-features" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Image Features</h2>

<h1>
<a id="future-work" class="anchor" href="#future-work" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Future Work</h1>

<p>When I reached this part of the course the lecture videos were taken down, as it took me an incredible amount of time to do this assignment with the lecture videos, my teacher and I decided to focus on a project or two that would both contribute to this course and prepare me for my summer internship. Here is a link to that repo and what exactly I finished my term with. However, I have not given up on this course, I still feel I have a tremendous amount to learn from it. But from my initial experience, I hope to revisit it some time next year after I have taken linear algebra and multivariable calculus so I am more capable of working through it independently. </p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/georgewildridge/CS231n">CS231n</a> is maintained by <a href="https://github.com/georgewildridge">georgewildridge</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
