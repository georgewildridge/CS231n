{
  "name": "CS231n",
  "tagline": "Following Stanford's course \"Convolutional Neural Networks for Visual Recognition\"",
  "body": "This will be a comprehensive review of everything I have learned along with a walk through of my code.  I will work from the big picture down to the nitty gritty of the code. If you have questions or spot a mistake feel free to tweet me at [@Gwildridge](https://twitter.com/Gwildridge). A brief disclaimer that I am a highschool student and although the information on this page is correct to the best of my knowledge, I would not consider myself all-knowing on this material. \r\n#Introduction\r\n##Computer Vision\r\nComputer vision is the problem that Deep Learning is attempting to solve and I am attempting to learn about. The approach to understanding this has also been an approach to understanding human vision. Over the past fifty years, our understanding of human vision has been widdled down from a holistic approach where some part of our brain stores what a chair looks like to a hierarchical understanding. Hierarchical vision revolves around the understanding that our brain sees edges and that these edges define the shape. So instead of storing a chair somewhere in our brains we store a set of edges and lines that make up a chair and our brain knows which objects are which upon recognition. The overarching goal is to build a 3d model of objects within the computer. \r\nThe first approach to solving vision was edge detection nad then moved into segmentation for feature detection. For almost sixty years, people attempted to solve vision using this method… until very recently when deep leaning began being used. Now this isn't to say that Deep Learning hadn't existed until recently, it is to say that very few focused on it as a solution because it was too computationally costly. As of 2010 deep learning was a gain popularized. \r\n\r\n## Image Classification\r\nImage classification is taking an image and transform it into one of a fixed number of categories. Vision is challenging because of a number of environment variables like rotation, point of view, illumination, deformation, occlusion, background clutter and finally interclass variation. There are two approaches, the explicit approach and the data driven approach. An example of the explicit approach is the feature detection, or the association of certain characteristics with certain objects. The issue with this approach is that every time you want to classify a new object a new algorithm must be hard coded to look for those characteristics, making this method incapable of major growth. Instead, a data driven has been taken with deep learning. This involves collecting a very large dataset of images and labels. Then machine learning is used to train and image classifier on the data prior to evaluating the classifier on the test images. It is this classifier I will address next. \r\n\r\n#Assignment 1\r\nIn this assignment I will implement the k-nearest neighbor classifier, a support vector machine, the softmax classifier, backpropagation, a neural net and generate some features. \r\n\r\n##K-Nearest Neighbor Classifier\r\nThis classifier works by comparing an image for similarities to ever other image in the dataset, and is classified based of the image(s)' class that it is most similar to. By similar I am refering to the the distance's between the two images. Two different methods can be used to find the distance, the L1/Manhattan distance:\r\n![Manhattan distance](link)\r\nAnd the L2/Euclidean distance:\r\n![Euclidean Distance](link)\r\nThe image with the smallest distance between the two is considered to be the most similar image. However, what I just described was the nearest neighbor classifier. The K-nearest neighbor classifier is very similar. Instead of just stating the image with the smallest number is the most similar, K of the most similar images are retrieved before selecting the class that the majority of the K images have. \r\n\r\nIn this particular assignment we were asked to implement this classifier using the Euclidean/L2 distance in three different ways: with two for loops, with one for loop and with no for loops. This is designed to introduce us to running loops in C instead of python.\r\n\r\n###Two Loops\r\nConverting the equation into numpy operations:\r\n\t\tnum_test = X.shape[0]\r\n\t\tnum_train = self.X_train.shape[0]\r\n\t\tdists = np.zeros((num_test, num_train))\r\n\t\tfor i in xrange(num_test):\r\n\t\t\tfor j in xrange(num_train):\r\n\t\t\t\tdists[i,j] = np.sqrt(np.sum(np.square(X[i]-self.X_train[j])))\r\n\t\treturn dists\r\nWhere X[i] is the image we are attempting to find the class for and X_train[j] is the remainder of the images in the training set.\r\n\r\n###One Loop\r\nReducing the same operation to one loop\r\n\t\tfor i in xrange(num_test):\r\n\t\t\tdists[i,:] = np.sqrt(np.sum(np.square(X[i,:]-self.X_train),axis =1))\r\n\t\treturn dists\r\nThe important thing to understand in this case is what exactly something like dists[i,:] is doing. From playing around with it and from the [python tutorial](http://cs231n.github.io/python-numpy-tutorial/), I figured out that his would represent every column in row i. So basically I am saying that every column in row i of dists is equal to the Euclidean distance of every column of row i minus every image in the dataset. \r\n###No Loops\r\nNow we take it a step further, computing the euclidean distance without loops:\r\n\t\tdists = np.sqrt((X**2).sum(axis=1)[:, np.newaxis] + (self.X_train**2).sum(axis=1) - 2 * X.dot(self.X_train.T))\r\n\t\treturn dists\r\nTo put this into perspective I will do it on a much simpler scale. Instead of dealing with these huge matricies, pretend we are just working with two vectors, q and p. To find the [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance) between these two vectors we find the length of its distance vector:\r\n![Euclidean length of distance vector](https://upload.wikimedia.org/math/c/5/4/c54c79bb419dec31d93260cb9207a1d5.png)\r\nOtherwise written as:\r\n![rewritten Euclidean length of distance vector](https://upload.wikimedia.org/math/a/8/3/a8394d4ad1d858186f89be9c590a3ac8.png)\r\nAlright, now that we understand how to find the length of the two vector's distance vector, the same process is applied to the bigger matrices and that’s it. \r\n\r\n\r\n##Support Vector Machine\r\n\r\n\r\n\t\t\tii. S is a vector of class scores, safety margin of 1 (scores are scale free, arbitrary choice). J is the incorrect classes why yi is the correct class.\r\n\t\t\tiii. It taking the scores of the incorrect classes subtracting the scores of the correct classes and adding one, next the max is taken of the two points 0 or the loss. This clamps values at zero (if a negative loss is associated, only a zero will be passed through)so the negative losses don’t have a bearing on the overall loss of the function as the negative loss would decrease the overall loss, making the loss seem smaller then it actually is. Importantly the loss score is not calculated when j = yi as this would give a value of one(where it should be 0) which has a non universal affect on the overall loss (bad). Finally all the losses over all the classes are averaged (see equation below) to find a single loss number.  (3:00 - 9:45)\r\n\t\t\t\t1) \r\n\t\t\tiv. Hyperparamter\r\n\t\t\t\t1) This is known as the hinge loss\r\n\t\t\t\t2) There is also the squared hinge loss (will change what is actually happens.. But occasionally works better):\r\n\t\t\t\t\ta) \r\n\t\t\tv. Now we have two theoretically working equation of: \r\n\t\t\t\t1) \r\n\t\t\t\t2) This would not implement correctly however because if the weights are universally multiplied by a constant the issue would arrise that if L equalled 0 and the weights were multiplied, L would still equal 0 by nature of this equation despite, using logic, the weights not having an equivalent distance from eachother (19:00 - 20:00)\r\n\t\t\t\t\ta) To solve this we introduce regularization\r\n\t\t\tvi. Weight Regularization –Not sold I am not sure I fully understand this\r\n\t\t\t\t1) \r\n\t\t\t\t2)  Lamda is a hyperparameter that allows you to adjust the strenght of regularization\r\n\t\t\t\t3)  Regularization measures the niceness of the w, trading off the training loss with the generalization loss. \r\n\t\t\t\t\ta) Generally L2 regulariation is used (see slide 23)\r\n\t\t\t\t\tb) Takes into account more data, diffuses the data, puts more emphasis on each point\r\n\t\t\t\t\tc) 9\r\n\r\n#Linear Classifier\r\n\t\ta. Parametric Approach:\r\n\t\t\ti. Constructing a function that takes an image and produces the scores for the classses (which class the image belongs to), the function is  a function of both the image and the parameters. The goal is for the funciton to provide a higher score for the correct labels, (labels a cat picture a cat) and a lower score for the rest of the classes. To do this we change the paramaters also know as the weights. \r\n\t\t\t\t1) \r\n\t\t\t\t2) X, being the image's pixels stretched into one column, W being the parameters and is the number of classes by the number of pixels in the image. Sometimes +b is added to the function to account for bias's (majority of the pictures in a dataset are cats, we want it to generally spit out cat then)\r\n\t\tb. Interpreting a Linear Classifer (4200)\r\n\t\t\ti. Every score is a weighted sum of all the pixel values of the image. It is counting up colors of all the pixel values of these positions.\r\n\t\t\tii. Classifiers create the optimal way of mixing all the test images to create a template that is \"compared\" to the image\r\n\r\n##Softmax Classifier\r\n\t\ta.  A different way to specify loss from the scores generated from the classifier or the neural net\r\n\t\tb.  Scores are interpretted as the \"unnormalized log probabilities of the classes\" so, instead of a loss function, we would basically just take the –log of the normalized scores after the scores are exponentiated to retrieve a number that we interpret as the probability of that specific class\r\n\t\t\ti. As a loss functionthegoal is to maximize the log liklihood of the true class and minimize the –log liklyhood of the true class \r\n\t\tc. As a sanity check, when you first initialize this with random small weights the svm loss should be around the –log(1/#number of classes), as optimizing that number should be decreasing to zero\r\nMaximizing log probabilities vs probablities... does the same thing. Log probs jst look nicer\r\n##Two-Layer Neural Network\r\n\r\n## Image Features\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}