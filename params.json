{
  "name": "CS231n",
  "tagline": "Following Stanford's course \"Convolutional Neural Networks for Visual Recognition\"",
  "body": "This will be a comprehensive review of everything I have learned along with a walk through of my code.  I will work from the big picture down to the nitty gritty of the code. If you have questions or spot a mistake feel free to tweet me at @GWildridge. \r\n\r\n##Computer Vision\r\nThis is the problem that Deep Learning is attempting to solve and I am attempting to learn about. The approach to understanding this has also been an approach to understanding human vision. Over the past fifty years, our understanding of human vision has been widdled down from a holistic approach where some part of our brain stores what a chair looks like to a hierarchical understanding. Hierarchical vision revolves around the understanding that our brain sees edges and that these edges define the shape. So instead of storing a chair somewhere in our brains we store a set of edges and lines that make up a chair. \r\nTo begin with, I would like to start with the issue that this work attempts to solve... Computer Vision. \r\n\r\n\r\n\r\n#Assignment 1\r\nIn this assignment I will implement the k-nearest neighbor classifier, a support vector machine, the softmax classifier, backprop, a neural net and generate some features. \r\n\r\nT\r\nTo begin with I want to talk about the prblem all of this is an attempt to solve... image classifcation. \r\n\r\n##K-Nearest Neighbor\r\nNearest Neighbor Classifier\r\n     a. Given a giant dataset\r\n\t\tb. Remember all the training data\r\n\t\tc. Compare a test image to every picture in the training data\r\n\t\t\ti. Images are classified based on their degree of similarity to other images in the dataset and the image with the most similarities class\r\n\t\td. How to compare the two pictures from the test and training datasets:\r\n\t\t\ti. Looking at the distances between pixels at the same location of the two images\r\n\t\t\tii. Hyperparameters: a discrete choice that the user has control over, not obvious how to set it, the user has to make the dicision on how to set it. Problem dependent, must try a range to see what works best\r\n\t\t\t\t1) Manhattan/L1 Distance: (11:25)\r\n\t\t\t\t\ta) \r\n\t\t\t\t\tb) Take the distance between the two pixels, then add the values of all the different pixels together to retrieve a final result, finally take the index of the training examples that has the lowest final result and apply that classifier to the test image\r\n\t\t\t\t2) Euclidean/L2 distance\r\n\t\t\t\t\ta) \r\n\t\t\t\t3) k-Nearest Neighbor\r\nRetrieve the k closest training images to the test image and select the class that the majority of the k images are closest to.\r\n\r\n\r\n##Support Vector Machine\r\n\t\t\ti. \r\n\t\t\tii. S is a vector of class scores, safety margin of 1 (scores are scale free, arbitrary choice). J is the incorrect classes why yi is the correct class.\r\n\t\t\tiii. It taking the scores of the incorrect classes subtracting the scores of the correct classes and adding one, next the max is taken of the two points 0 or the loss. This clamps values at zero (if a negative loss is associated, only a zero will be passed through)so the negative losses don’t have a bearing on the overall loss of the function as the negative loss would decrease the overall loss, making the loss seem smaller then it actually is. Importantly the loss score is not calculated when j = yi as this would give a value of one(where it should be 0) which has a non universal affect on the overall loss (bad). Finally all the losses over all the classes are averaged (see equation below) to find a single loss number.  (3:00 - 9:45)\r\n\t\t\t\t1) \r\n\t\t\tiv. Hyperparamter\r\n\t\t\t\t1) This is known as the hinge loss\r\n\t\t\t\t2) There is also the squared hinge loss (will change what is actually happens.. But occasionally works better):\r\n\t\t\t\t\ta) \r\n\t\t\tv. Now we have two theoretically working equation of: \r\n\t\t\t\t1) \r\n\t\t\t\t2) This would not implement correctly however because if the weights are universally multiplied by a constant the issue would arrise that if L equalled 0 and the weights were multiplied, L would still equal 0 by nature of this equation despite, using logic, the weights not having an equivalent distance from eachother (19:00 - 20:00)\r\n\t\t\t\t\ta) To solve this we introduce regularization\r\n\t\t\tvi. Weight Regularization –Not sold I am not sure I fully understand this\r\n\t\t\t\t1) \r\n\t\t\t\t2)  Lamda is a hyperparameter that allows you to adjust the strenght of regularization\r\n\t\t\t\t3)  Regularization measures the niceness of the w, trading off the training loss with the generalization loss. \r\n\t\t\t\t\ta) Generally L2 regulariation is used (see slide 23)\r\n\t\t\t\t\tb) Takes into account more data, diffuses the data, puts more emphasis on each point\r\n\t\t\t\t\tc) 9\r\n\r\n#Linear Classifier\r\n\t\ta. Parametric Approach:\r\n\t\t\ti. Constructing a function that takes an image and produces the scores for the classses (which class the image belongs to), the function is  a function of both the image and the parameters. The goal is for the funciton to provide a higher score for the correct labels, (labels a cat picture a cat) and a lower score for the rest of the classes. To do this we change the paramaters also know as the weights. \r\n\t\t\t\t1) \r\n\t\t\t\t2) X, being the image's pixels stretched into one column, W being the parameters and is the number of classes by the number of pixels in the image. Sometimes +b is added to the function to account for bias's (majority of the pictures in a dataset are cats, we want it to generally spit out cat then)\r\n\t\tb. Interpreting a Linear Classifer (4200)\r\n\t\t\ti. Every score is a weighted sum of all the pixel values of the image. It is counting up colors of all the pixel values of these positions.\r\n\t\t\tii. Classifiers create the optimal way of mixing all the test images to create a template that is \"compared\" to the image\r\n\r\n##Softmax Classifier\r\n\t\ta.  A different way to specify loss from the scores generated from the classifier or the neural net\r\n\t\tb.  Scores are interpretted as the \"unnormalized log probabilities of the classes\" so, instead of a loss function, we would basically just take the –log of the normalized scores after the scores are exponentiated to retrieve a number that we interpret as the probability of that specific class\r\n\t\t\ti. As a loss functionthegoal is to maximize the log liklihood of the true class and minimize the –log liklyhood of the true class \r\n\t\tc. As a sanity check, when you first initialize this with random small weights the svm loss should be around the –log(1/#number of classes), as optimizing that number should be decreasing to zero\r\nMaximizing log probabilities vs probablities... does the same thing. Log probs jst look nicer\r\n##Two-Layer Neural Network\r\n\r\n## Image Features",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}